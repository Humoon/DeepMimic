default: &default

  clip_range: 0.2
  action_entropy_parameter: 0.001
  learning_rate: 0.0001
  output_shape: 19
  pre_action_length: 1
  n_steps: 30   # rollout n_steps also is lstm steps, assert 3000 % n_steps == 0
  discount_rate: 0.997
  gae_lam: 0.95
  n_lstm: 256

#  share_feature_shape: 224
#  player_feature_shape: 110
#  player_num: 10  # remember uncomment myself feature
#  env_name: "11_vs_11_custom"

  share_feature_shape: 116
  player_feature_shape: 110
  player_num: 4
  total_player_num: 5
  env_name: "5_vs_5_custom"
  feature_processor: "features_v1"
  reward_processor: "rewarder_v2"
  football_model: "football_ppo_mlp"

#  share_feature_shape: 106
#  player_feature_shape: 54
#  player_num: 4
#  total_player_num: 5
#  player_shape: 8
#  env_name: "5_vs_5_custom"
#  feature_processor: "features_v0"
#  reward_processor: "rewarder_v1"
#  football_model: "football_ppo_attention"

  exp_name: "master_baseline"

# tensorboard log name will be: log_ + time_ + env_name_ + exp_name

# env_name (scenarios):
# 11_vs_11_easy_stochastic (player_num=1~11, difficulty=0.05)
# 11_vs_11_easy_custom (exclude goalkeeper, player_num=1~10, difficulty=0.05)
# 11_vs_11_competition (player_num=1~11, difficulty=1)
# 11_vs_11_custom (exclude goalkeeper, player_num=1~10, difficulty=1)
# 5_vs_5 (exclude goalkeeper, player_num=1~4, difficulty=0.05)
# 5_vs_5_custom (exclude goalkeeper, player_num=1~4, difficulty=1)



# feature shape example:
# share_feature_shape: 224
# player_feature_shape: 110
# player_num: 1～10
# env_name: "11_vs_11_custom"

# share_feature_shape: 116
# player_feature_shape: 110
# player_num: 1～4
# env_name: "5_vs_5"


development:
  <<: *default

production:
  <<: *default
